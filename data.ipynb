{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fbf2be2",
   "metadata": {},
   "source": [
    "# Data pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4f089-bb5f-4fa5-a16a-0da82c83c362",
   "metadata": {},
   "source": [
    "### Imports and paths initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c024d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the whole notebook\n",
    "from xml.etree import ElementTree as ET\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from owl2vec_star import owl2vec_star\n",
    "\n",
    "\n",
    "def parse_args(args=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Data pre-process',\n",
    "        usage=''\n",
    "    )\n",
    "    parser.add_argument('--data_dir', type=str, default='data')\n",
    "    parser.add_argument('--onto_dir', type=str, default='ontology')\n",
    "    parser.add_argument('--syn_data_dir', type=str, default='syn_data')\n",
    "    parser.add_argument('--onto_to_embed', type=str, default='hpObo_hoom_ordo.owl')\n",
    "    parser.add_argument('--embedding_cfg_path', type=str, default='./embedding.cfg')\n",
    "\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "\n",
    "# set the path to your data folders here\n",
    "args = parse_args(args=['--data_dir', '../persistent/data'])\n",
    "onto_dir_path = os.path.join(args.data_dir, args.onto_dir)\n",
    "syn_data_dir_path = os.path.join(args.data_dir, args.syn_data_dir)\n",
    "\n",
    "if not os.path.exists(onto_dir_path):\n",
    "    raise ValueError(f'You need an existing ontology directory with the XML dataset inside it, please create \\'{onto_dir_path}\\'')\n",
    "\n",
    "if not os.path.exists(syn_data_dir_path):\n",
    "    os.makedirs(syn_data_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606da578",
   "metadata": {},
   "source": [
    "### Convert XML dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1344a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: clinical signs and symptoms in rare diseases\n",
    "# http://www.orphadata.org/cgi-bin/index.php (Phenotypes associated with rare disorders)\n",
    "\n",
    "\n",
    "tree = ET.parse(os.path.join(onto_dir_path, 'en_product4.xml'))\n",
    "root = tree.getroot()\n",
    "\n",
    "\n",
    "headers = ['HPODisorderSetStatus_id', 'Disorder_id', 'OrphaCode', 'ExpertLink',\n",
    "           'Name', 'DisorderType_id', 'DisorderType_name', 'DisorderGroup_id',\n",
    "           'DisorderGroup_Name', 'HPODisorderAssociation_id', 'HPO_id',\n",
    "           'HPOId', 'HPOTerm', 'HPOFrequency_id', 'HPOFrequency_Name',\n",
    "           'DiagnosticCriteria_id', 'DiagnosticCriteria_Name', 'Source',\n",
    "           'ValidationStatus', 'Online', 'ValidationDate']\n",
    "\n",
    "\n",
    "def find_value(row_data, source_tag, target_tag_name, field, text=True):\n",
    "    \"\"\"Finds a sub-tag of a source tag and inputs its value into a dictionary\n",
    "        containing the current row's data\n",
    "\n",
    "    Args:\n",
    "        row_data (dict):\n",
    "            The data for the current row associated with the csv fields\n",
    "        source_tag (Element):\n",
    "            XML parent tag to search from\n",
    "        target_tag_name (str):\n",
    "            Name of the sub-tag to find\n",
    "        field (str):\n",
    "            Field in the csv file\n",
    "        text (bool):\n",
    "            Indicates if the value of the tag to retrieve is its inner text \n",
    "            or its id attribute\n",
    "    Returns:\n",
    "        tag (Element):\n",
    "            Returns the found tag\n",
    "    \"\"\"\n",
    "    tag = source_tag.find(target_tag_name)\n",
    "    tag_v = ''\n",
    "\n",
    "    if tag is not None:  # retrieving either the inner text or the id attribute of the tag\n",
    "        if text:\n",
    "            tag_v = tag.text\n",
    "        elif (len(tag.attrib) > 0):\n",
    "            tag_v = tag.attrib['id']\n",
    "    row_data[field] = tag_v if tag_v is not None else ''\n",
    "\n",
    "    return tag\n",
    "\n",
    "\n",
    "with open(os.path.join(onto_dir_path, 'en_product4.csv'), 'w', encoding='utf-8') as fd:\n",
    "    csvwriter = csv.DictWriter(fd, delimiter=',', fieldnames=headers)\n",
    "    csvwriter.writeheader()\n",
    "\n",
    "    # iterating through all the disorders\n",
    "    for status in root.find('HPODisorderSetStatusList').findall('HPODisorderSetStatus'):\n",
    "        row_data = {}\n",
    "        row_data['HPODisorderSetStatus_id'] = status.attrib['id']\n",
    "\n",
    "        disorder_tag = find_value(row_data, status, 'Disorder', 'Disorder_id', text=False)\n",
    "        find_value(row_data, disorder_tag, 'OrphaCode', 'OrphaCode', text=True)\n",
    "        find_value(row_data, disorder_tag, 'ExpertLink', 'ExpertLink', text=True)\n",
    "        find_value(row_data, disorder_tag, 'Name', 'Name', text=True)\n",
    "\n",
    "        disordertype_tag = find_value(row_data, disorder_tag, 'DisorderType', 'DisorderType_id', text=False)\n",
    "        find_value(row_data, disordertype_tag, 'Name', 'DisorderType_name', text=True)\n",
    "        disordergroup_tag = find_value(row_data, disorder_tag, 'DisorderGroup', 'DisorderGroup_id', text=False)\n",
    "        find_value(row_data, disordergroup_tag, 'Name', 'DisorderGroup_Name', text=True)\n",
    "\n",
    "        for field in ['Source', 'ValidationStatus', 'Online', 'ValidationDate']:\n",
    "            find_value(row_data, status, field, field, text=True)\n",
    "\n",
    "        # iterating through all the disorder associations and writing a row for each association\n",
    "        for association in disorder_tag.find('HPODisorderAssociationList').findall('HPODisorderAssociation'):\n",
    "            row_data['HPODisorderAssociation_id'] = association.attrib['id']\n",
    "\n",
    "            hpo_tag = find_value(row_data, association, 'HPO', 'HPO_id', text=False)\n",
    "            find_value(row_data, hpo_tag, 'HPOId', 'HPOId', text=True)\n",
    "            find_value(row_data, hpo_tag, 'HPOTerm', 'HPOTerm', text=True)\n",
    "            hpofrequency_tag = find_value(row_data, association, 'HPOFrequency', 'HPOFrequency_id', text=False)\n",
    "            find_value(row_data, hpofrequency_tag, 'Name', 'HPOFrequency_Name', text=True)\n",
    "\n",
    "            diagnosticcriteria_tag = find_value(row_data, association, 'DiagnosticCriteria', 'DiagnosticCriteria_id', text=False)\n",
    "            find_value(row_data, diagnosticcriteria_tag, 'Name', 'DiagnosticCriteria_Name', text=True)\n",
    "\n",
    "            csvwriter.writerow(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebdae01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HPODisorderSetStatus_id</th>\n",
       "      <th>Disorder_id</th>\n",
       "      <th>OrphaCode</th>\n",
       "      <th>ExpertLink</th>\n",
       "      <th>Name</th>\n",
       "      <th>DisorderType_id</th>\n",
       "      <th>DisorderType_name</th>\n",
       "      <th>DisorderGroup_id</th>\n",
       "      <th>DisorderGroup_Name</th>\n",
       "      <th>HPODisorderAssociation_id</th>\n",
       "      <th>...</th>\n",
       "      <th>HPOId</th>\n",
       "      <th>HPOTerm</th>\n",
       "      <th>HPOFrequency_id</th>\n",
       "      <th>HPOFrequency_Name</th>\n",
       "      <th>DiagnosticCriteria_id</th>\n",
       "      <th>DiagnosticCriteria_Name</th>\n",
       "      <th>Source</th>\n",
       "      <th>ValidationStatus</th>\n",
       "      <th>Online</th>\n",
       "      <th>ValidationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327485</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0000256</td>\n",
       "      <td>Macrocephaly</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327486</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0001249</td>\n",
       "      <td>Intellectual disability</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327487</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0001250</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327488</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0001257</td>\n",
       "      <td>Spasticity</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327489</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0001274</td>\n",
       "      <td>Agenesis of corpus callosum</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HPODisorderSetStatus_id  Disorder_id  OrphaCode  \\\n",
       "0                        1            2         58   \n",
       "1                        1            2         58   \n",
       "2                        1            2         58   \n",
       "3                        1            2         58   \n",
       "4                        1            2         58   \n",
       "\n",
       "                                          ExpertLink               Name  \\\n",
       "0  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "1  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "2  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "3  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "4  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "\n",
       "   DisorderType_id DisorderType_name  DisorderGroup_id DisorderGroup_Name  \\\n",
       "0            21394           Disease             36547           Disorder   \n",
       "1            21394           Disease             36547           Disorder   \n",
       "2            21394           Disease             36547           Disorder   \n",
       "3            21394           Disease             36547           Disorder   \n",
       "4            21394           Disease             36547           Disorder   \n",
       "\n",
       "   HPODisorderAssociation_id  ...       HPOId                      HPOTerm  \\\n",
       "0                     327485  ...  HP:0000256                 Macrocephaly   \n",
       "1                     327486  ...  HP:0001249      Intellectual disability   \n",
       "2                     327487  ...  HP:0001250                     Seizures   \n",
       "3                     327488  ...  HP:0001257                   Spasticity   \n",
       "4                     327489  ...  HP:0001274  Agenesis of corpus callosum   \n",
       "\n",
       "  HPOFrequency_id       HPOFrequency_Name DiagnosticCriteria_id  \\\n",
       "0           28412  Very frequent (99-80%)                   NaN   \n",
       "1           28412  Very frequent (99-80%)                   NaN   \n",
       "2           28412  Very frequent (99-80%)                   NaN   \n",
       "3           28412  Very frequent (99-80%)                   NaN   \n",
       "4           28412  Very frequent (99-80%)                   NaN   \n",
       "\n",
       "   DiagnosticCriteria_Name Source ValidationStatus Online  \\\n",
       "0                      NaN    NaN                y      y   \n",
       "1                      NaN    NaN                y      y   \n",
       "2                      NaN    NaN                y      y   \n",
       "3                      NaN    NaN                y      y   \n",
       "4                      NaN    NaN                y      y   \n",
       "\n",
       "          ValidationDate  \n",
       "0  2016-06-01 00:00:00.0  \n",
       "1  2016-06-01 00:00:00.0  \n",
       "2  2016-06-01 00:00:00.0  \n",
       "3  2016-06-01 00:00:00.0  \n",
       "4  2016-06-01 00:00:00.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6058d15c",
   "metadata": {},
   "source": [
    "### Dataset to triples, entities and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "793735e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_assoc = {  # from csv frequency to frequency code\n",
    "    'Obligate (100%)': 'O',\n",
    "    'Very frequent (99-80%)': 'VF',\n",
    "    'Frequent (79-30%)': 'F',\n",
    "    'Occasional (29-5%)': 'OC',\n",
    "    'Very rare (<4-1%)': 'VR',\n",
    "    'Excluded (0%)': 'E'\n",
    "}\n",
    "\n",
    "freq_code_assoc = {  # from frequency code to output class\n",
    "    'O': 'obligate',\n",
    "    'VF': 'very_frequent',\n",
    "    'F': 'frequent',\n",
    "    'OC': 'occasional',\n",
    "    'VR': 'very_rare',\n",
    "    'E': 'excluded'\n",
    "}\n",
    "\n",
    "dc_association = {  # default: exclusion\n",
    "    'Diagnostic criterion': 'diagnostic_criterion',\n",
    "    'Pathognomonic sign': 'pathognomonic_sign',\n",
    "}\n",
    "\n",
    "\n",
    "def get_association_subclass(orpha, freq, hp):\n",
    "    \"\"\"Returns normalized association class\n",
    "\n",
    "    Args:\n",
    "        orpha (str):\n",
    "            The prefixed Orphanet code\n",
    "        freq (str):\n",
    "            The frequency text\n",
    "        hp (str):\n",
    "            The prefixed HPO ID\n",
    "    Returns:\n",
    "        (str):\n",
    "            The orphacode, hpo id and frequency association\n",
    "    \"\"\"\n",
    "    return orpha + '_' + hp + '_FREQ:' + freq_assoc.get(freq)\n",
    "\n",
    "\n",
    "def get_association_name(orpha, freq, hp):\n",
    "    \"\"\"Returns textual description of the association class\n",
    "\n",
    "    Args:\n",
    "        orpha (str):\n",
    "            The prefixed Orphanet code\n",
    "        freq (str):\n",
    "            The frequency text\n",
    "        hp (str):\n",
    "            The prefixed HPO ID\n",
    "    Returns:\n",
    "        (str):\n",
    "            The orphacode, hpo id and frequency association \n",
    "            textual_description_with_underscores\n",
    "    \"\"\"\n",
    "    return get_normalized_string(orpha_entities.get(orpha) + ' and ' +\n",
    "                                 hpo_entities.get(hp) + ' ' +\n",
    "                                 freq_code_assoc.get(freq_assoc.get(freq)) +\n",
    "                                 ' association')\n",
    "\n",
    "\n",
    "def get_normalized_string(s):\n",
    "    \"\"\"Transforms a string to lowercase and replaces all whitespace runs with an underscore\n",
    "\n",
    "    Args:\n",
    "        s (str):\n",
    "            String to normalize\n",
    "    Returns:\n",
    "        (str):\n",
    "            The normalized string\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\s+\", '_', s.lower())\n",
    "\n",
    "\n",
    "df_dataset = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'), \n",
    "                         dtype='object')\n",
    "df_dataset['OrphaCode'] = df_dataset['OrphaCode'].map(lambda x: 'ORPHA:' + x)\n",
    "\n",
    "# key is id, value is textual_description_with_underscores\n",
    "assoc_entities = {}\n",
    "dc_entities = {'diagnostic_criterion': 'diagnostic_criterion',\n",
    "               'pathognomonic_sign': 'pathognomonic_sign',\n",
    "               'exclusion': 'exclusion'}\n",
    "freq_assoc_entities = {'obligate': 'obligate', 'very_frequent': 'very_frequent',\n",
    "                       'frequent': 'frequent', 'occasional': 'occasional',\n",
    "                       'very_rare': 'very_rare', 'excluded': 'excluded'}\n",
    "hpo_entities = {}\n",
    "orpha_entities = {}\n",
    "\n",
    "has_object_triples = []  # association has_object HPOId\n",
    "has_subject_triples = []  # association has_subject OrphaCode\n",
    "has_frequency_triples = []  # association has_frequency FrequencyAssociation\n",
    "has_diagnostic_criterion_triples = []  # association has_DC_attribute DC\n",
    "\n",
    "\n",
    "# reading the dataset\n",
    "for orpha, freq, hp, dc, \\\n",
    "    orpha_name, hpo_name in zip(df_dataset['OrphaCode'],\n",
    "                                df_dataset['HPOFrequency_Name'],\n",
    "                                df_dataset['HPOId'],\n",
    "                                df_dataset['DiagnosticCriteria_Name'],\n",
    "                                df_dataset['Name'],\n",
    "                                df_dataset['HPOTerm']):\n",
    "    if hp not in hpo_entities:\n",
    "        hpo_entities[hp] = get_normalized_string(hpo_name)\n",
    "    if orpha not in orpha_entities:\n",
    "        orpha_entities[orpha] = get_normalized_string(orpha_name)\n",
    "\n",
    "    ac = get_association_subclass(orpha, freq, hp)\n",
    "    ac_name = get_association_name(orpha, freq, hp)\n",
    "    assoc_entities[ac] = ac_name\n",
    "\n",
    "    has_object_triples.append((ac, 'association_has_object', hp))\n",
    "    has_subject_triples.append((ac, 'association_has_subject', orpha))\n",
    "    has_frequency_triples.append((ac, 'has_frequency', freq_code_assoc.get(freq_assoc.get(freq))))\n",
    "    has_diagnostic_criterion_triples.append((ac, 'has_DC_attribute', dc_association.get(dc, 'exclusion')))\n",
    "\n",
    "\n",
    "# lists corresponding to each output file\n",
    "triples = []\n",
    "triples_names = []\n",
    "entities = []\n",
    "entities_names = []\n",
    "relations = []\n",
    "\n",
    "# subClassOf triples\n",
    "entities_and_parent_class = [\n",
    "    (assoc_entities, 'association'),\n",
    "    (dc_entities, 'diagnostic_criteria'), \n",
    "    (freq_assoc_entities, 'frequency_association'), \n",
    "    (hpo_entities, 'HPO_Id'), \n",
    "    (orpha_entities, 'OrphaCode')\n",
    "]\n",
    "for (entities_dict, parent_class) in entities_and_parent_class:\n",
    "    for k, v in entities_dict.items(): \n",
    "        triples.append((k, 'subClassOf', get_normalized_string(parent_class)))\n",
    "        triples_names.append((v, 'subClassOf', parent_class))\n",
    "\n",
    "# other properties triples\n",
    "triples_and_entities = [\n",
    "    (has_object_triples, hpo_entities), \n",
    "    (has_subject_triples, orpha_entities), \n",
    "    (has_frequency_triples, freq_assoc_entities), \n",
    "    (has_diagnostic_criterion_triples, dc_entities)\n",
    "]\n",
    "for (properties_triples, entities_dict) in triples_and_entities:\n",
    "    for (s, r, o) in properties_triples:\n",
    "        triples.append((s, r, o))\n",
    "        triples_names.append((assoc_entities.get(s), r, entities_dict.get(o)))\n",
    "\n",
    "# parent entities\n",
    "for i, (k, v) in enumerate(entities_and_parent_class):\n",
    "    parent = get_normalized_string(v)\n",
    "    entities.append((i, parent))\n",
    "    entities_names.append((i, parent))\n",
    "# entities\n",
    "parents_count = len(entities)\n",
    "for i, (k, v) in enumerate({**assoc_entities, **dc_entities, **freq_assoc_entities,\n",
    "                            **hpo_entities, **orpha_entities}.items()):\n",
    "    entities.append((i+parents_count, k))\n",
    "    entities_names.append((i+parents_count, v))\n",
    "\n",
    "# relations\n",
    "for i, r in enumerate(['subClassOf', 'association_has_object',\n",
    "                       'association_has_subject', 'has_frequency', \n",
    "                       'has_DC_attribute']):\n",
    "    relations.append((i, r))\n",
    "\n",
    "\n",
    "# writing to the different files\n",
    "lists_and_files = [\n",
    "    (triples, 'triples.txt'), \n",
    "    (triples_names, 'triples_names.txt'),\n",
    "    (entities, 'entities.dict'),\n",
    "    (entities_names, 'entities_names.dict'),\n",
    "    (relations, 'relations.dict')\n",
    "]\n",
    "for (l, n) in lists_and_files:\n",
    "    with open(os.path.join(onto_dir_path, n), 'w') as f:\n",
    "        for t in l:\n",
    "            f.write('\\t'.join(str(e) for e in t) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70178f4",
   "metadata": {},
   "source": [
    "### Merge ORDO and HP ontologies using the dataset (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16c0a8c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../persistent/data/ontology/ORDO.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8494/2587233701.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_ordo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monto_dir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ORDO.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_hp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monto_dir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HP.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monto_dir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'en_product4.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    648\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../persistent/data/ontology/ORDO.csv'"
     ]
    }
   ],
   "source": [
    "df_ordo = pd.read_csv(os.path.join(onto_dir_path, 'ORDO.csv'), dtype='object')\n",
    "df_hp = pd.read_csv(os.path.join(onto_dir_path, 'HP.csv'), dtype='object')\n",
    "df_dataset = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'), dtype='object')\n",
    "\n",
    "\n",
    "# prefixes to distinguish the columns from the 2 ontologies\n",
    "df_ordo = df_ordo.add_prefix('ORDO_')\n",
    "df_hp = df_hp.add_prefix('HP_')\n",
    "\n",
    "# normalizing the different columns for the merge\n",
    "df_ordo['OrphaCode'] = df_ordo['ORDO_Class ID'].map(lambda x: x.replace('http://www.orpha.net/ORDO/Orphanet_', ''))\n",
    "df_hp['HPOId'] = df_hp['HP_http://www.w3.org/2004/02/skos/core#notation']\n",
    "df_dataset['OrphaCode'] = df_dataset['OrphaCode'].astype(str)\n",
    "\n",
    "# merge\n",
    "df_merged = pd.merge(df_dataset, df_hp, how='left', on='HPOId')\n",
    "df_merged = pd.merge(df_merged, df_ordo, how='left', on='OrphaCode')\n",
    "\n",
    "df_merged.head(1000).to_csv(os.path.join(onto_dir_path, 'merged_ontologies.csv'), encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f95eb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HPODisorderSetStatus_id</th>\n",
       "      <th>Disorder_id</th>\n",
       "      <th>OrphaCode</th>\n",
       "      <th>ExpertLink</th>\n",
       "      <th>Name</th>\n",
       "      <th>DisorderType_id</th>\n",
       "      <th>DisorderType_name</th>\n",
       "      <th>DisorderGroup_id</th>\n",
       "      <th>DisorderGroup_Name</th>\n",
       "      <th>HPODisorderAssociation_id</th>\n",
       "      <th>...</th>\n",
       "      <th>ORDO_http://www.w3.org/2004/02/skos/core#notation</th>\n",
       "      <th>ORDO_https://creativecommons.org/licenses/permits</th>\n",
       "      <th>ORDO_https://creativecommons.org/licenses/requires</th>\n",
       "      <th>ORDO_major susceptibility factor in</th>\n",
       "      <th>ORDO_manual_assertion</th>\n",
       "      <th>ORDO_modifying germline mutation in</th>\n",
       "      <th>ORDO_part of a fusion gene in</th>\n",
       "      <th>ORDO_part_of</th>\n",
       "      <th>ORDO_present_in</th>\n",
       "      <th>ORDO_role in the phenotype of</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327485</td>\n",
       "      <td>...</td>\n",
       "      <td>ORPHA:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_182070|http...</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_410102|http...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327486</td>\n",
       "      <td>...</td>\n",
       "      <td>ORPHA:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_182070|http...</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_410102|http...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327487</td>\n",
       "      <td>...</td>\n",
       "      <td>ORPHA:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_182070|http...</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_410102|http...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327488</td>\n",
       "      <td>...</td>\n",
       "      <td>ORPHA:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_182070|http...</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_410102|http...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327489</td>\n",
       "      <td>...</td>\n",
       "      <td>ORPHA:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_182070|http...</td>\n",
       "      <td>http://www.orpha.net/ORDO/Orphanet_410102|http...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  HPODisorderSetStatus_id Disorder_id OrphaCode  \\\n",
       "0                       1           2        58   \n",
       "1                       1           2        58   \n",
       "2                       1           2        58   \n",
       "3                       1           2        58   \n",
       "4                       1           2        58   \n",
       "\n",
       "                                          ExpertLink               Name  \\\n",
       "0  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "1  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "2  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "3  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "4  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "\n",
       "  DisorderType_id DisorderType_name DisorderGroup_id DisorderGroup_Name  \\\n",
       "0           21394           Disease            36547           Disorder   \n",
       "1           21394           Disease            36547           Disorder   \n",
       "2           21394           Disease            36547           Disorder   \n",
       "3           21394           Disease            36547           Disorder   \n",
       "4           21394           Disease            36547           Disorder   \n",
       "\n",
       "  HPODisorderAssociation_id  ...  \\\n",
       "0                    327485  ...   \n",
       "1                    327486  ...   \n",
       "2                    327487  ...   \n",
       "3                    327488  ...   \n",
       "4                    327489  ...   \n",
       "\n",
       "  ORDO_http://www.w3.org/2004/02/skos/core#notation  \\\n",
       "0                                          ORPHA:58   \n",
       "1                                          ORPHA:58   \n",
       "2                                          ORPHA:58   \n",
       "3                                          ORPHA:58   \n",
       "4                                          ORPHA:58   \n",
       "\n",
       "  ORDO_https://creativecommons.org/licenses/permits  \\\n",
       "0                                               NaN   \n",
       "1                                               NaN   \n",
       "2                                               NaN   \n",
       "3                                               NaN   \n",
       "4                                               NaN   \n",
       "\n",
       "  ORDO_https://creativecommons.org/licenses/requires  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "  ORDO_major susceptibility factor in ORDO_manual_assertion  \\\n",
       "0                                 NaN                   NaN   \n",
       "1                                 NaN                   NaN   \n",
       "2                                 NaN                   NaN   \n",
       "3                                 NaN                   NaN   \n",
       "4                                 NaN                   NaN   \n",
       "\n",
       "  ORDO_modifying germline mutation in ORDO_part of a fusion gene in  \\\n",
       "0                                 NaN                           NaN   \n",
       "1                                 NaN                           NaN   \n",
       "2                                 NaN                           NaN   \n",
       "3                                 NaN                           NaN   \n",
       "4                                 NaN                           NaN   \n",
       "\n",
       "                                        ORDO_part_of  \\\n",
       "0  http://www.orpha.net/ORDO/Orphanet_182070|http...   \n",
       "1  http://www.orpha.net/ORDO/Orphanet_182070|http...   \n",
       "2  http://www.orpha.net/ORDO/Orphanet_182070|http...   \n",
       "3  http://www.orpha.net/ORDO/Orphanet_182070|http...   \n",
       "4  http://www.orpha.net/ORDO/Orphanet_182070|http...   \n",
       "\n",
       "                                     ORDO_present_in  \\\n",
       "0  http://www.orpha.net/ORDO/Orphanet_410102|http...   \n",
       "1  http://www.orpha.net/ORDO/Orphanet_410102|http...   \n",
       "2  http://www.orpha.net/ORDO/Orphanet_410102|http...   \n",
       "3  http://www.orpha.net/ORDO/Orphanet_410102|http...   \n",
       "4  http://www.orpha.net/ORDO/Orphanet_410102|http...   \n",
       "\n",
       "  ORDO_role in the phenotype of  \n",
       "0                           NaN  \n",
       "1                           NaN  \n",
       "2                           NaN  \n",
       "3                           NaN  \n",
       "4                           NaN  \n",
       "\n",
       "[5 rows x 127 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'), dtype='object')\n",
    "df_res = pd.read_csv(os.path.join(onto_dir_path, 'merged_ontologies.csv'), dtype='object')\n",
    "\n",
    "df_res.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d9077-ef7a-45c5-b5f3-c6361b8a9000",
   "metadata": {},
   "source": [
    "### Synthetic data generation from the ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06240382-5159-4e7d-8e3e-e35997620c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 unique rare diseases, 192 unique phenotypes\n",
      "0/100 patients generated\n",
      "100/100 patients generated, writing to files\n",
      "Number of columns in syn_patients_data_seen.csv: 194\n",
      "Number of columns in syn_patients_data_unseen.csv: 193\n",
      "Total RDs: 10 seen RDs: 8, unseen RDs: 2\n"
     ]
    }
   ],
   "source": [
    "frequency_dict = {  # frequency ids + associated probability\n",
    "    28405: 1,  # Obligate (100%)\n",
    "    28412: 0.895,  # Very frequent (99-80%)\n",
    "    28419: 0.545,  # Frequent (79-30%)\n",
    "    28426: 0.17,  # Occasional (29-5%)\n",
    "    28433: 0.025,  # Very rare (<4-1%)\n",
    "    28440: 0  # Excluded (0%)\n",
    "}\n",
    "\n",
    "\n",
    "def get_normalized_string(s):\n",
    "    \"\"\"Transforms a string to lowercase and replaces all whitespace runs with an underscore\n",
    "\n",
    "    Args:\n",
    "        s (str):\n",
    "            String to normalize\n",
    "    Returns:\n",
    "        (str):\n",
    "            The normalized string\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\s+\", '_', s.lower())\n",
    "\n",
    "\n",
    "def gen_syn_data(patients_per_rd=10, unseen_pct=0.2, gen_small_file=True, print_every=0):\n",
    "    \"\"\"Generates synthetic seen and unseen data from the ontology into files\n",
    "\n",
    "    Args:\n",
    "        patients_per_rd (int):\n",
    "            Number of generated patients per RD\n",
    "        unseen_pct (float):\n",
    "            The percentage of RDs to keep for unseen patients samples\n",
    "        gen_small_file (bool):\n",
    "            Whether to generate very small files to debug the model or the full ones\n",
    "        print_every (int):\n",
    "            Each number of patients to print progress during data generation (0 = no print)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'))\n",
    "\n",
    "    if gen_small_file:\n",
    "        grouped = df.groupby('Name', sort=False)\n",
    "        df = pd.concat([group for name, group in grouped][:10])\n",
    "        grouped = df.groupby('Name', sort=False)\n",
    "    else:\n",
    "        # randomizing the order of the RDs for randomized seen/unseen RDs\n",
    "        df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "        grouped = df_shuffled.groupby('Name', sort=False)\n",
    "\n",
    "    rd_count = grouped.ngroups\n",
    "    unique_hps = df.HPOTerm.unique()\n",
    "    total_hp_count = len(unique_hps)\n",
    "\n",
    "    print(f'{rd_count} unique rare diseases, {total_hp_count} unique phenotypes')\n",
    "\n",
    "    phenotypes = unique_hps.tolist()\n",
    "    phenotypes_dict = {hp: i for i, hp in enumerate(phenotypes)}\n",
    "    seen_unseen_th = rd_count - math.ceil(rd_count*unseen_pct)  # threshold of RDs to switch to unseen RDs\n",
    "\n",
    "    seen_patients_data = [['patient_id', 'rare_disease'] + phenotypes]\n",
    "    unseen_patients_data = [['patient_id'] + phenotypes]\n",
    "\n",
    "    distribution_check = {  # key: count of patients with hp + maximum count of patients that could've had the hp\n",
    "        28405: [0, 0],  # Obligate (100%)\n",
    "        28412: [0, 0],  # Very frequent (99-80%)\n",
    "        28419: [0, 0],  # Frequent (79-30%)\n",
    "        28426: [0, 0],  # Occasional (29-5%)\n",
    "        28433: [0, 0],  # Very rare (<4-1%)\n",
    "        28440: [0, 0]  # Excluded (0%)\n",
    "    }\n",
    "\n",
    "    patients_count = 0\n",
    "    lists_n_files = [\n",
    "        (seen_patients_data, 'syn_patients_data_seen.csv'),\n",
    "        (unseen_patients_data, 'syn_patients_data_unseen.csv')\n",
    "    ]\n",
    "\n",
    "    for group_nb, (name, group) in enumerate(grouped):  # for each RD\n",
    "        hp_count = len(group)\n",
    "        # generate patients_per_rd patients for each RD\n",
    "        for patient_id in range(patients_count, patients_count+patients_per_rd):\n",
    "            temp_hp = []\n",
    "            proba_results = np.random.rand(hp_count)  # generating random floats for probabilities\n",
    "            rd = ''\n",
    "            for i, (rd_name, hp_name, frequency_id) in enumerate(zip(\n",
    "                                                            group['Name'],\n",
    "                                                            group['HPOTerm'],\n",
    "                                                            group['HPOFrequency_id'])):\n",
    "                distribution_check.get(frequency_id)[1] += 1\n",
    "                if rd == '':\n",
    "                    rd = rd_name\n",
    "                if (proba_results[i] >= 1 - frequency_dict[frequency_id]):  # comparing generated float and proba\n",
    "                    temp_hp.append(hp_name)\n",
    "                    distribution_check.get(frequency_id)[0] += 1\n",
    "\n",
    "            if len(temp_hp) > 0:\n",
    "                row = np.zeros((total_hp_count,), dtype=int)\n",
    "                if (group_nb <= seen_unseen_th):\n",
    "                    for hp in temp_hp:\n",
    "                        row[phenotypes_dict.get(hp)] = 1\n",
    "                    seen_patients_data.append(np.concatenate([[patient_id], [rd], row]))\n",
    "                else:\n",
    "                    for hp in temp_hp:\n",
    "                        row[phenotypes_dict.get(hp)] = 1\n",
    "                    unseen_patients_data.append(np.concatenate([[patient_id - math.ceil(rd_count*unseen_pct)], row]))\n",
    "\n",
    "                if print_every > 0:\n",
    "                    if patients_count % print_every == 0:\n",
    "                        print(f'{patients_count}/{patients_per_rd*rd_count} patients generated')\n",
    "\n",
    "                patients_count += 1\n",
    "\n",
    "    print(f'{len(seen_patients_data)-1} seen patients generated, {len(unseen_patients_data)-1} unseen patients generated, writing to files')\n",
    "\n",
    "    # writing the 2 files\n",
    "    for lst, fn in lists_n_files:\n",
    "        print(f'Number of columns in {fn}: {len(lst[0])}')\n",
    "        if gen_small_file:\n",
    "            fn = 'small_' + fn\n",
    "        pd.DataFrame(lst).to_csv(\n",
    "            os.path.join(syn_data_dir_path, fn),\n",
    "            encoding='utf-8', index=False, header=False\n",
    "        )\n",
    "\n",
    "    print(f'Total RDs: {rd_count} seen RDs: {seen_unseen_th}, unseen RDs: {rd_count-seen_unseen_th}')\n",
    "\n",
    "    return distribution_check\n",
    "\n",
    "\n",
    "distributions = gen_syn_data(patients_per_rd=10, unseen_pct=0, gen_small_file=True, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2efc1076-f81d-4e2d-bd27-a749b25ff15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(28405, [6100, 6100]), (28412, [231503, 258920]), (28419, [204675, 374800]), (28426, [69615, 411400]), (28433, [1631, 64140]), (28440, [0, 7050])]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqk0lEQVR4nO3de7wXVb3/8debDbpBUY9IHhQEKjQ1lRQ0U5PEW2pa3vWQYh1JzVt5vKT+POSlk1l5skxT08K8poWUdBSPqZU3wNAQxSvKVjwpFqCg3D6/P9baMHzde/PduL97b5j38/H4PvZ818ysWWtm9nxmzcx3jSICMzMrry4dXQAzM+tYDgRmZiXnQGBmVnIOBGZmJedAYGZWcg4EZmYl50DQxiSNlvSrji5Ha0gaKenPLYz/kqSZkt6R9Kn2LFsZSPqFpIs7uhxNkXSupOs+ZB4DJIWkrs2M30LSFEnzJJ36YZZlq8aBoJXyQfNvkuZLekPSVZI26Ohy1dj3gZMjYt2I+GtHF6Y9SXpW0leaSD9N0qQ8vLWkeyW9LemfkiZL2q+Z/FoMuh1J0jBJDcW0iPhORPx7jRd9FvDHiOgZEVfUeFnWBAeCVpB0BnApcCawPvBpoD8wQdJa7ViOJs+saqg/8HQnKUt7+yVwTBPpX87jAH4HTAD+FfgIcCowt11K1wqdeFs1u38BSKprx7KUU0T4U8UHWA94Bzi8In1d4E3gK/n7aOAO4DZgHvAEsF1h+rOB1/K46cDwnN4FOAd4EZgN3A5smMcNAAL4KvAq8BDwB9JZerEsTwIH5+FPkA5Ob+flHF6YrhcwjnSwehy4CPhzE3VeO9c5gHeBF3P6jFyPp4D3ga6koPgw8M9cjmGFfAYCD+Y6TwB+AvwqjxsGNFQsdwawZyvWy7F5vbwFnFfIpw44N887D5gM9AOuBH5QscxxwDeaWAd9gcVA/0LaVsBCYKP8CWCDKvahLYH3gCV5vf4zp/8il+nuXM7HgI8V5mtpW64PjCHtg68A5wNd8riRwF+Ay/O6uzhv0+/n9fV/wNVAd2AdYAGwNJftHWAT0v78q8Lydi1s55nAyJy+P/BX0j41ExhdmKdxO3VtYp3cn9fHe3mZm+f1cRUwnrTf7ZnLcmeu58vAqYU8uud5/gFMI52oNRTGB/DxwvdfABcXvh8ATMl1ehjYtmJf/A/Svj6H9H9dXxh/UJ53Lmk/2xc4DJhcUc9vAnd19HGs2X2zowuwunzyBl7czM78S+CWPDwaWAQcCnTLO9HLeXiL/E+ySZ52APkfHjgNeJR04Fkb+Fkhz8Z/pDH5H7Y76Sz1L4UybJV35LXzNDOB40gH6U+RDpJb5WlvJR1Q1wE+SQpMHwgEhbwr/5Fm5J2/Xy7LpqQDzX6kA/de+XvvPP0jwA9z2T5LOthVGwiqWS/X5nJsRwpMW+bxZwJ/y+tdeXwvYEfgdZYfMDcC5gMbN1P/CcD5he//BYzNwwKeB34PfLG5PArzjqxc16QD0+xcrq7ATcCtedzKtuUY4C6gZ14fzwFfLSxrMXBKnrc7KSiMAzbM8/wO+K8WtsXowrbqn7fdUaT9uRcwuDDvNnn7b0sKMl+s2E4f+N/J4x8A/r1ifcwBdsn59SAF8QuAtYCPAi8B++Tpvwv8KdepHzCVKgNBXp9/B3YinTgcS9r/1i7si4+TAtGGwDPACXncjrmce+VybkoK2muTgvaWhWX+FTiko49jze6XHV2A1eUDjADeaGbcd4EJeXg08GhhXBdgFrAb8PG80+0JdKvI4xly6yB/70MKKF0L/0gfLYzvSTpb6p+/XwJcn4ePAP5Ukf/PgP/MO/si4BOFcd+h9YHgK4XvZwM3VsxzT/6n2ox0MFqnMO5mqg8E1ayXvoXxjwNH5uHpwEHN1OkZYK88fDIwfiXbfnphe74KfKkwvi+plfMi6Yz6IWBQM3mNrFzXpAPTdYXv+wHPVrktF5KDQh73NeCBwrJeLYxT3meKrY2dgZdb2BajC9vqW8Bvq/x/+W/g8jzcuJ1aEwjGFL7vVKxHoSw35OGXgH0L40ZRfSC4CrioIu/pwO6FfXFEYdz3gKsL2+HyZup0FXBJHt6a1FpZu5p11xEf3yOo3lvARs1cZ+2Txzea2TgQEUuBBlIr4AXgdNI/198l3Sppkzxpf+C3+WbjP0kHqiXAxs3kO490KeHInHQU6UyyMa+dGvPK+f0b6Rp2b9JBdFlepEsKrVWcvz9wWMXydiWtl02Af0TEu6u4vGrWyxuF4fmky3WQzg5fbCbfX5IO8OS/N7ZQht8AfSR9mnSw7EFa9wBERENEnBwRH8vlfZd0pt4azdWhpW25EenMvLg+XyGdmTYqbqfeueyTC3n9T06vRrPrU9JOkv4o6U1Jc4ATcvlWVeX+tUnFOjiX5fvAJqz6/twfOKMi7345z0arun8dLUmk+0m3R8T7rShXu3IgqN4jpMsOBxcTJa0LfB7430Jyv8L4LqQzxtcBIuLmiNiVtAMG6eYzpB358xGxQeFTHxGvFfKNijLdAhwlaWegHvhjIa8HK/JaNyJOJF1jXVwsI+msvbWKZZlJahEUl7dORHyX1Br6F0nrNLO8d0kHJ2DZjcHigama9dKcmcDHmhn3K+AgSduRrt2PbS6TiJhPuu9zDOmf+taIWNjMtDNJ1/s/2Vx2VZS7qKVt+RapddS/MP1mpEt9TS3vLdJ9gK0Lea0fEes2MW1zZWlufd5MuuTULyLWJ917UDUVbEbl/vVyxTroGRGNT2bNouX9eT6FfYwURIt5X1KRd4+IuKWKMja7PiLiUVJrbTfgaFo+0ehwDgRViog5wLeBH0vaV1I3SQNI19obWHFD7yDp4Nx6OJ0UQB7Nz0vvIWlt0s2xxptzkP5xLpHUH0BSb0kHraRY40kHgQuB23LrA9L16s0lfTmXs5ukoZK2jIglpDPc0ZJ6SNqKdAnnw/gV8AVJ+0iqk1SfH0XsGxGvAJOAb0taS9KuwBcK8z4H1EvaX1I30s3OtQvjV2W9NLoOuEjSICXbSuoF6SwemEjabndGxIKV5PVL0mWaQ1j+tBCS/kXStyV9XFIXSRsBXyHd12jK/wF9W/GU2cq25e2k9dMzr6NvkrbHB+T941rgckkfyeXfVNI+hbL1krR+M2W5CdhT0uGSukrqJWlwHtcTeDsi3pO0I+ng11YeB+ZJOltS97yPfVLS0Dz+duBbeVv0Jd0TKZpCOjuvk7QvsHth3LXACblFI0nr5H2xZxXl+jlwnKThedtvKukThfFjSJcMF0VEp3xkuJEDQStExPdITdLvk54SeIx0VjC8otl3F+mg8Q/SGeTBEbGIdID7LunM7A3So4bfyvP8iHRGda+keaQDyU4rKc/7pIP6nqQzssb0ecDepMtGr+dlXcryA+zJpObtG6TrpTe0akV8sBwzSU9PnEtqccwk3aht3L+OznV5m3Rte0xh3jnASaSD9mukFkLxWfZWr5eCH5IOEveSttfPSTdMG/2SdIOzmrO1h0g3BhsiYmIhfSHpGvh9eRlTSYF/ZDP53E96VPINSW81M80yVWzLU0jr7CXgz6T94PoWsjwbeIF0YjI3l3uLvKxnSa3Ml/JlkuLlESLiVdL9izNI23IK6QY8pG14Yd5GF5DWe5vIAe8AYDDpwYu3SPtLY8D6Nuly0MukbV25PU8jnXz8k3RZbWwh70nA8aQD9j9I62ZkleV6nHQT/3LSvvEgK7bObiS1DDv9D0yVb2aYtRtJo0k370asbNoal+OzpH/S/uF/hDWGpGGkG9x9O7gc3UkPh2wfEc93ZFlWxi0CK6V8Geo00tM6DgJWCycCEzt7EID09IhZqUjaknTf4klS096sTUmaQbpZ/sWOLUl1fGnIzKzkfGnIzKzkVrtLQxtttFEMGDCgo4thZrZamTx58lsR0eSPB1e7QDBgwAAmTZrU0cUwM1utSGr2F9e+NGRmVnIOBGZmJedAYGZWcqvdPQIzW/0sWrSIhoYG3nvvvY4uyhqvvr6evn370q1bt6rncSAws5praGigZ8+eDBgwgNQzs9VCRDB79mwaGhoYOHBg1fP50pCZ1dx7771Hr169HARqTBK9evVqdcurZoFA0vWS/i5pajPjJekKSS9IekrS9rUqi5l1PAeB9rEq67mWLYJfkN7z25zPA4PyZxTp1W5mZtbOanaPICIeyi9uac5BpPeSBqlv9A0k9YmIWbUqk5l1DgPOuXvlE7XCjO/uX9V0DQ0NfP3rX2fatGksXbqUAw44gMsuu4ybb76ZSZMm8ZOf/KRNy/Vhrbvuurzzzjs1X05H3izelBXfM9qQ0z4QCCSNIrUa2GyzVXmrYtKWO9+M+jZ8AdPoOW2Xl5k1KSI4+OCDOfHEE7nrrrtYsmQJo0aN4rzzzmPrrbdu8+UtXryYrl1Xj+dxVoubxRFxTUQMiYghvXtX+55tM7Pl7r//furr6znuuNTzeF1dHZdffjnXX3898+fPZ+bMmQwbNoxBgwbx7W9/G4B3332X/fffn+22245PfvKT3HbbbQBMnjyZ3XffnR122IF99tmHWbPS+euwYcM4/fTTGTJkCJdccgn9+/dn6dKly/Lq168fixYt4sUXX2Tfffdlhx12YLfdduPZZ58F4OWXX2bnnXdmm2224fzzz2+3ddOR4eo1VnzhdF9WfOm2mVmbefrpp9lhhx1WSFtvvfXYbLPNWLx4MY8//jhTp06lR48eDB06lP33359XXnmFTTbZhLvvTlcT5syZw6JFizjllFO466676N27N7fddhvnnXce11+f3hC6cOHCZf2hPfHEEzz44IN87nOf4/e//z377LMP3bp1Y9SoUVx99dUMGjSIxx57jJNOOon777+f0047jRNPPJFjjjmGK6+8st3WTUe2CMYBx+Snhz4NzPH9ATPrKHvttRe9evWie/fuHHzwwfz5z39mm222YcKECZx99tn86U9/Yv3112f69OlMnTqVvfbai8GDB3PxxRfT0LD8NdtHHHHECsONrYhbb72VI444gnfeeYeHH36Yww47jMGDB/O1r31tWYviL3/5C0cddRQAX/7yl9ut7jVrEUi6BRgGbCSpgfTS8m4AEXE1MJ70IuwXgPn4TVFmVkNbbbUVd9xxxwppc+fO5dVXX6Vr164feOxSEptvvjlPPPEE48eP5/zzz2f48OF86UtfYuutt+aRRx5pcjnrrLPOsuEDDzyQc889l7fffpvJkyezxx578O6777LBBhswZcqUJufviMdsa9YiiIijIqJPRHSLiL4R8fOIuDoHASL5ekR8LCK2iQj3LW1mNTN8+HDmz5/PmDFjAFiyZAlnnHEGI0eOpEePHkyYMIG3336bBQsWMHbsWHbZZRdef/11evTowYgRIzjzzDN54okn2GKLLXjzzTeXBYJFixbx9NNPN7nMddddl6FDh3LaaadxwAEHUFdXx3rrrcfAgQP59a9/DaSb2E8++SQAu+yyC7feeisAN910U61XyTKrxy1tM1ujVPu4Z1uSxG9/+1tOOukkLrroIpYuXcp+++3Hd77zHW655RZ23HFHDjnkEBoaGhgxYgRDhgzhnnvu4cwzz6RLly5069aNq666irXWWos77riDU089lTlz5rB48WJOP/30Zp88OuKIIzjssMN44IEHlqXddNNNnHjiiVx88cUsWrSII488ku22244f/ehHHH300Vx66aUcdNBB7bRmVsN3Fg8ZMiRW9cU0fnzUrGM888wzbLnllh1djNJoan1LmhwRQ5qafrV4fNTMzGrHgcDMrOQcCMzMSs6BwMys5BwIzMxKzoHAzKzk/DsCM2t/o9dv4/xafgR79uzZDB8+HIA33niDuro6evfuzYwZM9hkk02YNm1a25anBWPHjmXzzTdnq622AuCCCy7gs5/9LHvuuWer8pkxYwYHHHAAU6c2+e6vVnGLwMzWeL169WLKlClMmTKFE044gW984xvLvnfp0vaHwcWLFzc7buzYsSsEngsvvLDVQaCtORCYWaktWbKE448/nq233pq9996bBQsWADTbVfSMGTPYY4892HbbbRk+fDivvvoqACNHjuSEE05gp5124qyzzmpy/ocffphx48Zx5plnMnjwYF588UVGjhy5rA+kiRMn8pnPfIbtttuOHXfckXnz5jFjxgx22203tt9+e7bffnsefvjhNl8HvjRkZqX2/PPPc8stt3Dttddy+OGHc+eddzJixIhmu4o+5ZRTOPbYYzn22GO5/vrrOfXUUxk7diyQ3oD28MMPU1dXx/Dhw5uc/8ADD+SAAw7g0EMPXaEcCxcuXNZb6dChQ5k7dy7du3fnIx/5CBMmTKC+vp7nn3+eo446ilXtXaE5DgRmVmoDBw5k8ODBAOywww7MmDFjha6iG73//vsAPPLII/zmN78BUlfRZ5111rJpDjvsMOrq6lqcvznTp0+nT58+DB06FEjvSoD0QpuTTz6ZKVOmUFdXx3PPPffhK13BgcDMSm3ttddeNlxXV8eCBQtYunRpi11FN6exC+pVnb8pl19+ORtvvDFPPvkkS5cupb6+/kPnWcn3CMzMKrTUVfRnPvOZFbqK3m233Vo1f8+ePZk3b94H5tliiy2YNWsWEydOBGDevHksXryYOXPm0KdPH7p06cKNN97IkiVL2ry+bhHYcm35SJ97VLWWrAb7R3NdRf/4xz/muOOO47LLLqN3797ccMMNrZr/yCOP5Pjjj+eKK65Y4UU5a621FrfddhunnHIKCxYsoHv37tx3332cdNJJHHLIIYwZM4Z99913hRfftBV3Q72KOlM31G1Vr85UJ1uzuBvq9uVuqM3MrFUcCMzMSs6BwMzaxep2GXp1tSrr2YHAzGquvr6e2bNnOxjUWEQwe/bsVj9i6qeGzKzm+vbtS0NDA2+++WZHF2WNV19fT9++fVs1jwOBmdVct27dGDhwYEcXw5rhS0NmZiXnQGBmVnIOBGZmJedAYGZWcg4EZmYl50BgZlZyDgRmZiXnQGBmVnIOBGZmJVfTQCBpX0nTJb0g6Zwmxm8m6Y+S/irpKUn71bI8Zmb2QTULBJLqgCuBzwNbAUdJ2qpisvOB2yPiU8CRwE9rVR4zM2taLVsEOwIvRMRLEbEQuBU4qGKaANbLw+sDr9ewPGZm1oRaBoJNgZmF7w05rWg0MEJSAzAeOKWpjCSNkjRJ0iT3Xmhm1rY6+mbxUcAvIqIvsB9wo6QPlCkiromIIRExpHfv3u1eSDOzNVktA8FrQL/C9745reirwO0AEfEIUA9sVMMymZlZhVoGgonAIEkDJa1Fuhk8rmKaV4HhAJK2JAUCX/sxM2tHNQsEEbEYOBm4B3iG9HTQ05IulHRgnuwM4HhJTwK3ACPD77IzM2tXNX1DWUSMJ90ELqZdUBieBuxSyzKYmVnLOvpmsZmZdTAHAjOzknMgMDMrOQcCM7OScyAwMys5BwIzs5JzIDAzKzkHAjOzknMgMDMrOQcCM7OScyAwMys5BwIzs5JzIDAzKzkHAjOzknMgMDMrOQcCM7OScyAwMys5BwIzs5JzIDAzKzkHAjOzknMgMDMrOQcCM7OScyAwMys5BwIzs5JzIDAzKzkHAjOzknMgMDMrOQcCM7OSqyoQSNqm1gUxM7OOUW2L4KeSHpd0kqT1a1oiMzNrV1UFgojYDfg3oB8wWdLNkvaqacnMzKxdVH2PICKeB84HzgZ2B66Q9Kykg2tVODMzq71q7xFsK+ly4BlgD+ALEbFlHr68hfn2lTRd0guSzmlmmsMlTZP0tKSbV6EOZmb2IXStcrofA9cB50bEgsbEiHhd0vlNzSCpDrgS2AtoACZKGhcR0wrTDAK+BewSEf+Q9JFVrIeZma2iagPB/sCCiFgCIKkLUB8R8yPixmbm2RF4ISJeyvPcChwETCtMczxwZUT8AyAi/r4KdTAzsw+h2nsE9wHdC9975LSWbArMLHxvyGlFmwObS/qLpEcl7VtleczMrI1U2yKoj4h3Gr9ExDuSerTR8gcBw4C+wEOStomIfxYnkjQKGAWw2WabtcFizcysUbUtgnclbd/4RdIOwIIWpgd4jfS4aaO+Oa2oARgXEYsi4mXgOVJgWEFEXBMRQyJiSO/evassspmZVaPaFsHpwK8lvQ4I+FfgiJXMMxEYJGkgKQAcCRxdMc1Y4CjgBkkbkS4VvVRlmczMrA1UFQgiYqKkTwBb5KTpEbFoJfMslnQycA9QB1wfEU9LuhCYFBHj8ri9JU0DlgBnRsTsVa2MmZm1XrUtAoChwIA8z/aSiIgxLc0QEeOB8RVpFxSGA/hm/piZWQeoKhBIuhH4GDCFdOYOEECLgcDMzDq/alsEQ4Ct8hm8mZmtQap9amgq6QaxmZmtYaptEWwETJP0OPB+Y2JEHFiTUpmZWbupNhCMrmUhzMys41T7+OiDkvoDgyLivvyr4rraFs3MzNpDtU8NHU/q4mFD0tNDmwJXA8NrVzQrswHn3N1mec2or/wd4yoaPadt8jHrZKq9Wfx1YBdgLix7SY27jDYzWwNUGwjej4iFjV8kdSX9jsDMzFZz1QaCByWdC3TP7yr+NfC72hXLzMzaS7WB4BzgTeBvwNdI3UY0+WYyMzNbvVT71NBS4Nr8MTOzNUi1Tw29TBP3BCLio21eIjMza1et6WuoUT1wGOlRUjMzW81VdY8gImYXPq9FxH+TXmhvZmaruWovDW1f+NqF1EJozbsMzMysk6r2YP6DwvBiYAZweJuXxszM2l21Tw19rtYFMTOzjlHtpaEWXyUZET9sm+KYmVl7a81TQ0OBcfn7F4DHgedrUSgzM2s/1QaCvsD2ETEPQNJo4O6IGFGrgpmZWfuotouJjYGFhe8Lc5qZma3mqm0RjAEel/Tb/P2LwC9rUiIzM2tX1T41dImkPwC75aTjIuKvtSuWmZm1l2ovDQH0AOZGxI+ABkkDa1QmMzNrR1UFAkn/CZwNfCsndQN+VatCmZlZ+6m2RfAl4EDgXYCIeB3oWatCmZlZ+6k2ECyMiCB3RS1pndoVyczM2lO1geB2ST8DNpB0PHAffkmNmdkaYaVPDUkScBvwCWAusAVwQURMqHHZzMysHaw0EERESBofEdsAPvibma1hqr009ISkoTUtiZmZdYhqf1m8EzBC0gzSk0MiNRa2rVXBzMysfbQYCCRtFhGvAvusSuaS9gV+BNQB10XEd5uZ7hDgDmBoRExalWWZmdmqWVmLYCyp19FXJN0ZEYdUm7GkOuBKYC+gAZgoaVxETKuYridwGvBYq0puZmZtYmX3CFQY/mgr894ReCEiXoqIhcCtwEFNTHcRcCnwXivzNzOzNrCyQBDNDFdjU2Bm4XtDTltG0vZAv4i4u6WMJI2SNEnSpDfffLOVxTAzs5as7NLQdpLmkloG3fMwLL9ZvN6qLlhSF+CHwMiVTRsR1wDXAAwZMqS1AcnMzFrQYiCIiLoPkfdrQL/C9745rVFP4JPAA+k3a/wrME7Sgb5hbGbWflrTDXVrTQQGSRooaS3gSJa/85iImBMRG0XEgIgYADwKOAiYmbWzmgWCiFgMnAzcAzwD3B4RT0u6UNKBtVqumZm1TrU/KFslETEeGF+RdkEz0w6rZVnMzKxptbw0ZGZmqwEHAjOzknMgMDMrOQcCM7OScyAwMys5BwIzs5JzIDAzKzkHAjOzknMgMDMrOQcCM7OScyAwMys5BwIzs5JzIDAzKzkHAjOzknMgMDMrOQcCM7OScyAwMys5BwIzs5JzIDAzKzkHAjOzknMgMDMrOQcCM7OScyAwMys5BwIzs5JzIDAzKzkHAjOzknMgMDMrOQcCM7OScyAwMys5BwIzs5JzIDAzKzkHAjOzkqtpIJC0r6Tpkl6QdE4T478paZqkpyT9r6T+tSyPmZl9UM0CgaQ64Erg88BWwFGStqqY7K/AkIjYFrgD+F6tymNmZk2rZYtgR+CFiHgpIhYCtwIHFSeIiD9GxPz89VGgbw3LY2ZmTahlINgUmFn43pDTmvNV4A9NjZA0StIkSZPefPPNNiyimZl1ipvFkkYAQ4DLmhofEddExJCIGNK7d+/2LZyZ2Rquaw3zfg3oV/jeN6etQNKewHnA7hHxfg3LY2ZmTahli2AiMEjSQElrAUcC44oTSPoU8DPgwIj4ew3LYmZmzahZIIiIxcDJwD3AM8DtEfG0pAslHZgnuwxYF/i1pCmSxjWTnZmZ1UgtLw0REeOB8RVpFxSG96zl8s3MbOU6xc1iMzPrODVtEZjZcgPOubvN8ppRf3Sb5cXoOW2Xl62W3CIwMys5BwIzs5JzIDAzKzkHAjOzknMgMDMrOQcCM7OScyAwMys5BwIzs5JzIDAzKzkHAjOzknMgMDMrOQcCM7OScyAwMys5BwIzs5JzIDAzKzkHAjOzknMgMDMrOQcCM7OScyAwMys5BwIzs5JzIDAzKzkHAjOzknMgMDMrOQcCM7OScyAwMys5BwIzs5JzIDAzKzkHAjOzknMgMDMrua4dXQAzW70NOOfuNstrRv3RbZPR6Dltk09JuEVgZlZyNQ0EkvaVNF3SC5LOaWL82pJuy+MfkzSgluUxM7MPqlkgkFQHXAl8HtgKOErSVhWTfRX4R0R8HLgcuLRW5TEzs6bVskWwI/BCRLwUEQuBW4GDKqY5CPhlHr4DGC5JNSyTmZlVUETUJmPpUGDfiPj3/P3LwE4RcXJhmql5mob8/cU8zVsVeY0CRuWvWwDTa1Lo1tkIeGulU61e1sQ6wZpZrzWxTrBm1quz1Kl/RPRuasRq8dRQRFwDXNPR5SiSNCkihnR0OdrSmlgnWDPrtSbWCdbMeq0OdarlpaHXgH6F731zWpPTSOoKrA/MrmGZzMysQi0DwURgkKSBktYCjgTGVUwzDjg2Dx8K3B+1ulZlZmZNqtmloYhYLOlk4B6gDrg+Ip6WdCEwKSLGAT8HbpT0AvA2KVisLjrVpao2sibWCdbMeq2JdYI1s16dvk41u1lsZmarB/+y2Mys5BwIzMxKzoEAkHS9pL/n3zU0pm0oaYKk5/Pff8npknRF7hbjKUnb5/QtJE3OaTvntK6S7pPUox3qcJqkqZKelnR6TttO0iOS/ibpd5LWa2bewZIelTRF0iRJO3Z0XfMyphQ+cyWd3oo6XZTLN0XSvZI26eg6NVHGfpL+KGla3m6n5fRq6zha0muFdbRfTt8l12OSpEE5bYO8HjrF/3zeBu+0MP4SSTMrp5F0St7Px+eHUJC0q6TLa13mlVETXepIuilvi+8Upjtf0hc7rKBNiYjSf4DPAtsDUwtp3wPOycPnAJfm4f2APwACPg08ltN/COxKekz2zpx2CjCyHcr/SWAq0IP0AMB9wMdJT27tnqf5CnBRM/PfC3y+UL8HOlNdSQ8bvAH0b0Wd1isMnwpc3ZnqlPPvA2yfh3sCz5G6Y6m2jqOB/2gi/Te5HrsCP8hp3weG1XpfbKacawHrFL4PAW4E3mlhnk/n9fNORfqjpBPY84Ev5O14D7BhR9StYh99Efhoru+TwLbAdXn8BNLj8X2A33VkWZv6dIqzg44WEQ+RnloqKnZ/8Uvgi4X0MZE8CmwgqQ+wiHQg7gEskrQBaUcdU9vSA7Al6YA2PyIWAw8CBwObAw/laSYAhzQzfwCNZ53rA6/n4c5S1+HAixHxClXWKSLmFr6uQ6ojdJ46ERGzIuKJPDwPeAbYlOq3W3Mq6/IxoF9EPNAW5a6WpC0l/YDUE8DmOa0OuAw4q6V5I+LRiJjVVLZAN3LdgBHAHyKi8v+3vTXVpc7+QPfcCusGLAEuBP6z44rZtNXil8UdZOPCjvgGsHEe3hSYWZiuIaddSTporA18Dfh/wHciYmk7lHUqcImkXsAC0lnvJOBp0oFvLHAYK/7Ar+h04B5J3yedbX0mp3eWuh4J3JKHq60Tki4BjgHmAJ/LyZ2lTpVlHQB8CniMVtQROFnSMaTtfUZE/AP4L1JdFgBfJrUGzq9V2YskrQMcTupQEuAGYHQOdAAnA+MiYpZWrVuxn5BaBU8DfwHuAvb5UIVuG03tVzsBbwJPkFpAHwe6NAb/zsQtgipEatu1+JxtRLwaEcMiYmdgPqlp/oykG5W62t68huV7htRz673A/wBTSGcfXwFOkjSZdOlhYTNZnAh8IyL6Ad8g/b6jpeW1W13zdeADgV/npGrrREScl+t0E+kA1KyO3H6S1gXuBE7PLZlq63gV8DFgMDAL+EGuy5SI+HREfI50qWJWWoxuk/QrSRs3k19bmEUKAv8eEbtGxM8bg0C+T3MY8ONVzTwiboyIT0XECNK+egXweUl3SLq8s9wDaRQRp0fE4Ij4AXAR8P8knSfpdknHd3T5lunoa1Od5QMMYMV7BNOBPnm4DzA9D/8MOKqp6QpptwGDgEuA3UnXtm9qx7p8BzipIm1z4PE8fAMpWIzP3+ew/DclAuZ2lrqSzozvbWZcs3WqmG6zxm3bGepUsaxupGvc3/yQdVxh/y1sy3uBDUnBsH+uzyU13Pf2zutvGnABqaOzxnH7k1rXM/JnKfAC6fr6lPy5sCK/Ju8jAJsAv8/DD+Y8/hPYq1Z1W0m9dwbuKXz/FvCtiv14dN6e1+e0e4AeHVHeyk+nip6dTLH7i2NJTdDG9GPy0yefBuZE4VqmpN2B1yPiedJ1zKX5U9MnhyR9JP/djHR/4OZCWuPNtasBIuK4SGcp++XZXycdIAD2AJ7Pw52hrkex/LIQ1dap8WmZ7CDg2U5Up8ZlidT6eiYifrgKdexTyO5LpEuERceQgsbbta5Lo4i4NyKOAHYjnWDclZ+8GhARd0fEv0bEgIgYAMyPiI9HxJJcr8ERcUGVi7qIFGgAupNa7DX/P2tBs13qSOpGuvz6PZaXFVLwWqv9i9qEjo5EneFDOtDMIt18aiA1bXsB/0s6KN5HfiqBdJZ1JekJgb8BQwr5iHRzr3HaLUnXB58CdqlxHf5EOgt7Ehie004jPYnyHPBd8ll/E/PuCkzO8z4G7NAZ6kq6yTsbWL+QVm2d7iQdGJ8Cfgds2hnq1MR6j5z/lPzZrxV1vDHX4SnSQadPYVwP4I9At/x9tzztZGCLdv7/2pF0s7oyvaWnhr6X/xeX5r+jC+M+Bfy88P100j2D/wHWbs+6VZR5v7zNXgTOqyjfyMI+dkveFpd2VFkrP+5iwsys5HxpyMys5BwIzMxKzoHAzKzkHAjMzErOgcDMrOQcCKzTkLREK/Y4OqCjy9QRJI3Mv8I1axfua8g6kwURMbipEfnHV4p26vung40k/Qbi9ZVMt8okdY3UQaGZWwTWeUkakPt3H0M6MPaTdKakibmP928Xpj1P0nOS/izpFkn/kdMfkDQkD28kaUYerpN0WSGvr+X0YXmeOyQ9m/uTVx43VNLDkp6U9LiknpIekjS4UI4/S9quoh51kr6v1I/+U5JOyekX5OVPlXRN/rXzoaRumm/KraLuknaQ9KDS+xLuafxFcS5P4zsXLlN+n4akekk3KL3P4K+SPpfTR0oaJ+l+4H8ljVGhX/xc14PachvaaqKjf9Hmjz+NH1JHeVPy57ek/nOWAp/O4/cmvQhcpJOY35PeJbED6ZeaPUjdab9A7qcfeID862FgI2BGHh4FnJ+H1yb13jkQGEbqGqFvXsYjpF8ArwW8BAzN86xHalEfC/x3TtscmNREvU4E7gC65u8bFv/m4RuBLzRR5m7Aw0Dv/P0IlvdVMxXYOQ9/l+X9KZ1RmOYTwKtAPaml0VBY/u7A2Dy8PvByYxn9KdfHl4asM1nh0lC+R/BKpPcGQAoEewN/zd/XJXUO1xP4bUTMz/ONq2JZewPb5jNwSAfCQaSePh+PiIac1xRSQJoDzIqIibD8fQeSfk3qUfJMUq+hv2hiWXuSXoyzOM/b2Hf+5ySdRQpgG5K6SfhdxbxbkF48NCE3TOqAWUrvS+gZEY/k6W4GDsjDu5J7+IyIZyU1vscBYELj8iPiQUk/ldSb9M6DO8OXi0rJgcA6u3cLwwL+KyJ+VpxA+dWczVjM8kug9RV5nRIR91TkNQx4v5C0hBb+TyJivqQJpI7tDie1TlZKUj3wU9KZ/0xJoyvKVyzn05G6xy7Ov0E1y2nCuxXfx5Be7nIkcNwq5mmrOd8jsNXJPcBXlPrvR9KmuafOh4Av5uvpPUlvFms0g+UH50Mr8jox9wyJpM2VXqrSnOlAH0lD8/Q9JTUGiOtI/eJPjPRimEoTgK81Ti9pQ5Yf9N/K9SmWbR6pldO43N5a/h7lbpK2joh/AvMk7ZSnO7Iw/5+Af2usF6kb7unN1OsXpE7RiIhpLdTf1mBuEdhqIyLulbQl8Ei+TPIOMCIinpB0G6n31L+TugRu9H3gdkmjgLsL6deRLvk8kW8Gv8ny15E2teyFko4AfiypO+ntX3uSetCcLGku6X0BTbmOdGnmKUmLgGsj4ieSriVd53+josy/AK6WtIDUz/2hwBWS1if9z/436TLSV4FrJS0l9ck/J8//U+AqSX8jtYhGRsT7auKNYBHxf5KeIb0NzUrKvY/aGidfZnknIr7fTsvbhHSD9xPRjo+3Slo3It7Jw+eQuqE+rZV59CDdaN8+IuasbHpbM/nSkNmHoPS+4MdI/c+3928c9s+Pjk4lvW/g4tbMLGlP4Bngxw4C5eYWgZlZyblFYGZWcg4EZmYl50BgZlZyDgRmZiXnQGBmVnL/H2DVdoHHlj/WAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_distributions(frequency_obs, frequency_th):\n",
    "\n",
    "    index = np.arange(len(frequency_obs))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    fix, ax = plt.subplots()\n",
    "    summer = ax.bar(index, frequency_obs, bar_width, label='Observed')\n",
    "\n",
    "    winter = ax.bar(index+bar_width, frequency_th, bar_width, label='Theoretical')\n",
    "\n",
    "    ax.set_xlabel('Frequency category')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Observed frequency VS theoretical frequency')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(['100%', '99-80%', '79-30%', '29-5%', '<4-1%', '0%'])\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "sorted_items = sorted(distributions.items(), key=lambda x: x[0])\n",
    "print(sorted_items)\n",
    "frequency_obs = [t[1][0]/t[1][1] if t[1][0] != 0 else t[1][0] for t in sorted_items]\n",
    "sorted_freq = sorted(frequency_dict.items(), key=lambda x: x[0])\n",
    "frequency_th = [x[1] for x in sorted_freq]\n",
    "show_distributions(frequency_obs, frequency_th)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdfde1c",
   "metadata": {},
   "source": [
    "### ORDO and HPO IRIs dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a01ce23c-0768-4088-bafe-c6b58d96bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized names to ORDO IRIs and HPO IRIs\n",
    "\n",
    "df = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'))\n",
    "\n",
    "df_hp = df[['HPOTerm', 'HPOId']].drop_duplicates()\n",
    "df_rd = df[['Name', 'OrphaCode']].drop_duplicates()\n",
    "df_hp['HPOId'] = 'http://purl.obolibrary.org/obo/' + df_hp['HPOId'].str.replace(':', '_')\n",
    "df_rd['OrphaCode'] = 'http://www.orpha.net/ORDO/Orphanet_' + df_rd['OrphaCode'].astype(str)\n",
    "\n",
    "df_hp.to_csv(os.path.join(onto_dir_path, 'HPO.dict'), sep=';', encoding='utf-8', index=False, header=False)\n",
    "df_rd.to_csv(os.path.join(onto_dir_path, 'ORDO.dict'), sep=';', encoding='utf-8', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8253c004",
   "metadata": {},
   "source": [
    "### Merge ORDO, HP and HOOM ontologies using Protégé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce210777",
   "metadata": {},
   "source": [
    "https://bioportal.bioontology.org/ontologies/ORDO?p=summary \n",
    "    \n",
    "https://bioportal.bioontology.org/ontologies/HP?p=summary \n",
    "\n",
    "https://bioportal.bioontology.org/ontologies/HOOM?p=summary\n",
    "'HOOM is a module that qualifies the annotation between a clinical entity and phenotypic abnormalities according to a frequency and by integrating the notion of diagnostic criterion.'\n",
    "\n",
    "Using Protégé, merge HP (in OBO format, very important) into HOOM (OWL) and then ORDO (OWL) into the result of the merge, to obtain a merge of the 3 ontologies, export in OWL/XML format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d4ece",
   "metadata": {},
   "source": [
    "### Embedding the ontology with Owl2Vec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d6bf219-f3cf-4cb2-9d9b-38de47d1606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed47500c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"OWL2Vec-Star/OWL2Vec_Standalone.py\", line 6, in <module>\n",
      "    import gensim\n",
      "ModuleNotFoundError: No module named 'gensim'\n"
     ]
    }
   ],
   "source": [
    "output_folder = '../persistent/data/ontology/embeddings/hpObo_hoom_ordo'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    ontology_file\n",
    "    config_file\n",
    "    uri_doc\n",
    "    lit_doc\n",
    "    mix_doc\n",
    "    -> modify the cfg file for more params (cache dir, epochs, etc.)\n",
    "\"\"\"\n",
    "gensim_model = owl2vec_star.extract_owl2vec_model(None, \"./embedding.cfg\", True, True, True)\n",
    "\n",
    "# Gensim format\n",
    "gensim_model.save(os.path.join(output_folder, 'ontology.embeddings'))\n",
    "\n",
    "# Text format (not required)\n",
    "gensim_model.wv.save_word2vec_format(os.path.join(output_folder, \"ontology.embeddings.txt\"), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471bcc2c-22cb-494b-96a8-c15c60787a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding vectors generated above\n",
    "model = KeyedVectors.load(output_folder+\"ontology.embeddings\", mmap='r')\n",
    "wv = model.wv\n",
    "\n",
    "word = 'http://www.orpha.net/ORDO/Orphanet_556985'\n",
    "vector = wv[word]  # Get numpy vector of a word\n",
    "print(f\"Vector for {word}: {vector}\")\n",
    "\n",
    "#Most similar cosine similarity\n",
    "result1 = wv.most_similar(positive=[word])\n",
    "print(result1)\n",
    "\n",
    "#Most similar entities: cosmul\n",
    "result2 = wv.most_similar_cosmul(positive=[word])\n",
    "print(result2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
